{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\Data\\Customer Sent data\\Customer_sentiment.csv\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]: PART 1. IMPORT AND FUNCTIONS\n",
    "#region\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import gdown\n",
    "from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from numpy.core.defchararray import count\n",
    "from tensorflow import keras\n",
    "assert sys.version_info >= (3, 5)  # Python ≥3.5 is required\n",
    "assert tf.__version__ >= \"2.0\"  # TensorFlow ≥2.0 is required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for GPU usage:\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarations and functions for data preprocessing (used in Part 2 & Part 4)\n",
    "eos_id = 0  # end-of-seq token id\n",
    "sos_id = 1  # start-of-seq token id\n",
    "oov_id = 2  # out-of-vocab word id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word, vocab_list):\n",
    "    if word in vocab_list:\n",
    "        return vocab_list.index(word)\n",
    "    else:\n",
    "        return oov_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_word(id, vocab_list):\n",
    "    return vocab_list[id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_comment, Y_label=None, for_training=False, quite=True):\n",
    "    '''\n",
    "    Preprocess data.\n",
    "    Input: X_comment: list of strings (comments)\n",
    "           Y_label: list of labels (0,1,2). Required when for_training=True\n",
    "           for_training: bool. If True: generate vocab and stuff\n",
    "           quite=False: print some processed comments\n",
    "    Ouput: X_processed: tokenized and padded.\n",
    "           X_filtered, Y_filtered: some samples become empty after tokenization. The remaining X, Y are returned  (only returned when for_training=True)\n",
    "           vocab_X_size: size of vocab (only returned when for_training=True)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Delete all \\n:\n",
    "    X_comment = [i.replace('\\n',' ') for i in X_comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "665f7486a0f19aac68be06d26cdbb867b0983be08005ea0170db9c54d3acc11a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
